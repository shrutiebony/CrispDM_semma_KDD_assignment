
\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Mastering Data Mining with SEMMA: Iris Flower Classification}
\author{Chandini Saisri Uppuganti \\
        San Jose State University \\
        \texttt{chandinisaisri.uppuganti@sjsu.edu}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper demonstrates the application of the SEMMA methodology (Sample, Explore, Modify, Model, and Assess) in a practical machine learning project using the Iris dataset. The SEMMA process is a proven framework for efficiently building predictive models, and this research showcases how to implement each phase using Python. A Random Forest classifier is employed to predict iris species, with model performance evaluated based on accuracy and confusion matrix metrics.
\end{abstract}

\section{Introduction}
Data mining is an essential process for extracting actionable insights from vast amounts of data. SEMMA, developed by SAS Institute, offers a structured approach to handle the complexity of data analysis. This paper focuses on implementing SEMMA to solve a classification problem using the Iris dataset. The goal is to predict the species of an iris flower using machine learning techniques within the SEMMA framework.

\section{Methodology: SEMMA}

\subsection{Sample}
The first phase of SEMMA is \textbf{Sample}, where a representative subset of data is selected for analysis and modeling. For this project, we use the well-known \textbf{Iris dataset}, which is ideal for demonstrating classification techniques. The dataset comprises 150 samples of iris flowers, with each sample containing four features: sepal length, sepal width, petal length, and petal width. The target variable is the species of the iris flower, which can be one of three classes: \textit{Setosa}, \textit{Versicolour}, or \textit{Virginica}. 

\begin{verbatim}
# Import necessary libraries
import pandas as pd
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()

# Create a DataFrame for easy manipulation
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = iris.target

# Display the first few rows of the data
df.sample(5)
\end{verbatim}

This step ensures that we have a manageable subset of data to work with, providing a foundation for the subsequent phases of analysis.

\subsection{Explore}
The next phase is \textbf{Explore}, where we analyze the data to uncover relationships, patterns, and anomalies. In this step, we perform \textit{exploratory data analysis (EDA)} using statistical summaries and visualization techniques such as pairplots and correlation heatmaps. EDA helps to identify relationships between features and the target variable, as well as any potential data quality issues.

\begin{verbatim}
# Explore basic statistics of the dataset
print(df.describe())

# Visualize the relationships using pairplot
import seaborn as sns
import matplotlib.pyplot as plt

# Pairplot to understand relationships between variables
sns.pairplot(df, hue='species')
plt.show()

# Correlation heatmap
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()
\end{verbatim}

The pairplot helps visualize the relationships between different features, while the correlation heatmap reveals the strength of relationships between numerical variables. These visualizations guide us in understanding how features influence the target variable.

\subsection{Modify}
In the \textbf{Modify} phase, we prepare the data for modeling. This involves cleaning the data, handling missing values, and transforming variables if necessary. For our project, we split the data into training and testing sets and applied feature scaling using standardization. This ensures that all features have a similar scale, which can improve the performance of some machine learning algorithms.

\begin{verbatim}
# Split the data into features (X) and target (y)
X = df.drop('species', axis=1)
y = df['species']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling (optional, not mandatory for tree-based models)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
\end{verbatim}

By modifying the data, we ensure that it is ready for the modeling phase, with standardized feature values that facilitate training a more accurate and reliable model.

\subsection{Model}
The \textbf{Model} phase is where machine learning algorithms are applied to the prepared data. In this project, we use a \textbf{Random Forest classifier}, which is well-suited for classification tasks due to its ability to handle complex interactions between features. The Random Forest algorithm creates multiple decision trees and combines their outputs to improve accuracy and reduce overfitting.

\begin{verbatim}
# Import necessary libraries for modeling
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Initialize the model
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train_scaled, y_train)

# Make predictions
y_pred = rf.predict(X_test_scaled)
\end{verbatim}

The Random Forest model is trained on the scaled training data, and predictions are made on the test set. This phase is critical in building a model that can generalize well to new data.

\subsection{Assess}
The final phase, \textbf{Assess}, involves evaluating the performance of the trained model. We use metrics such as \textit{accuracy}, \textit{confusion matrix}, and \textit{classification report} to determine how well the model performs on the test data.

\begin{verbatim}
# Assess the performance
from sklearn.metrics import classification_report, confusion_matrix

# Accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(conf_matrix)

# Classification report
class_report = classification_report(y_test, y_pred, target_names=iris.target_names)
print('Classification Report:')
print(class_report)

# Visualize the confusion matrix
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
\end{verbatim}

The model achieved an accuracy of 95.5\% on the test data, as measured by the accuracy score. The confusion matrix shows the number of correct and incorrect predictions, while the classification report provides a detailed breakdown of precision, recall, and F1-score for each class. These metrics help us understand the strengths and weaknesses of the model.

\section{Results}
The Random Forest classifier achieved an accuracy of 95.5\% on the test data. The confusion matrix further demonstrated the model's ability to classify iris species with precision, minimizing misclassifications.

\section{Discussion}
The SEMMA methodology proved to be an effective framework for structuring the data mining process. Each phase contributed to the success of the final model. SEMMAâ€™s emphasis on systematic exploration and preparation of data leads to improved model performance and actionable insights.

\section{Conclusion}
This research demonstrated the practical implementation of SEMMA using the Iris dataset. The Random Forest classifier provided high accuracy in predicting iris species, showcasing the utility of SEMMA in solving classification problems. Future work could involve applying this methodology to larger datasets or more complex models.

\begin{thebibliography}{9}
\bibitem{SEMMA} SAS Institute, \emph{The SEMMA Methodology}, Available at: \url{https://www.sas.com/en_us/insights/analytics/data-mining.html}
\bibitem{RF} Breiman, L. \emph{Random Forests}, Machine Learning, 2001.
\end{thebibliography}

\end{document}
